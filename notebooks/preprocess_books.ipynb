{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T17:38:49.064595Z",
     "start_time": "2019-05-22T17:38:48.936979Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import LancasterStemmer, SnowballStemmer\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T00:19:36.835102Z",
     "start_time": "2019-05-20T00:19:36.828184Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_sent(book, num_sentences=20):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes in a book and preprocesses it so that it is ready to be vectorized and fed into a model. \n",
    "    Proper nouns are removed, along with punctuation, numbers, extra spaces.\n",
    "    The output is a list of lists containing the specified number of sentences each. \n",
    "    \n",
    "    Parameters:\n",
    "    book (str): an entire book in string format\n",
    "    num_sentences (int): number of sentences you want to group together\n",
    "    cpus (int): number of cpus to use for multiprocessing\n",
    "    \n",
    "    Returns:\n",
    "    processed_book (list): lists of sentences (default is 20) fully pre-processed\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_book = re.sub('\\\\n', ' ', book)    # substitute all newline characters with spaces\n",
    "    processed_book = word_tokenize(processed_book)    # word tokenize to identify proper nouns\n",
    "    # remove proper nouns and join back up into one string\n",
    "    processed_book = ' '.join([word for word,tag in pos_tag(processed_book) if not tag == 'NNP' or tag == 'NNPS'])\n",
    "    processed_book = sent_tokenize(processed_book)    # sentence tokenize \n",
    "    processed_book = [sentence.lower() for sentence in processed_book]    # make everything lowercase\n",
    "    processed_book = [re.sub('[^a-zA-Z]', ' ', sentence) for sentence in processed_book]    # take out all punctuation\n",
    "    processed_book = [re.sub('\\w*\\d\\w*', ' ', sentence) for sentence in processed_book]    # take out all numbers\n",
    "    processed_book = [re.sub('[  ]+', ' ', sentence) for sentence in processed_book]    # remove extra spaces\n",
    "    #processed_book = stem_words(processed_book)\n",
    "    processed_book = lemmatize_words(processed_book)\n",
    "    processed_book = [' '.join(processed_book[i:i+num_sentences]) for i in range(0, len(processed_book), num_sentences)]   # make lists of 20 sentences \n",
    "    processed_book = [re.sub('[  ]+', ' ', sentence) for sentence in processed_book]    # remove extra spaces\n",
    "    return processed_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T23:31:17.005137Z",
     "start_time": "2019-05-15T23:31:17.000609Z"
    }
   },
   "outputs": [],
   "source": [
    "def stem_words(book, method='snowball'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Stems words using Lancaster or Snowball stemming methods from the NLTK library.\n",
    "    \n",
    "    Parameters:\n",
    "    book (list): sentence-tokenized book\n",
    "    method (str): 'lancaster' or 'snowball' stemming styles\n",
    "    \n",
    "    Returns:\n",
    "    stemmed_book (list): sentences fully stemmed\n",
    "    \"\"\"\n",
    "    \n",
    "    stemmed_book = book.copy()\n",
    "    if method == 'lancaster':\n",
    "        stemmer = LancasterStemmer()\n",
    "    else:\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        \n",
    "    for i, sentence in enumerate(stemmed_book):\n",
    "        sentence_words = nltk.word_tokenize(sentence)\n",
    "        for j, word in enumerate(sentence_words):\n",
    "            sentence_words[j] = stemmer.stem(word)\n",
    "        stemmed_book[i] = ' '.join(sentence_words)\n",
    "    \n",
    "    return stemmed_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T00:02:16.244756Z",
     "start_time": "2019-05-16T00:02:16.240527Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_words(book):\n",
    "        \n",
    "    \"\"\"\n",
    "    Lemmatizes words using spaCy.\n",
    "    \n",
    "    Parameters:\n",
    "    book (list): sentence-tokenized book\n",
    "    method (str): 'lancaster' or 'snowball' stemming styles\n",
    "    \n",
    "    Returns:\n",
    "    stemmed_book (list): sentences fully lemmatized\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    lemmatized_book = book.copy()    \n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    for ix, sentence in enumerate(lemmatized_book):\n",
    "        sp_sent = []\n",
    "        for word in sp(sentence):\n",
    "            sp_sent.append(word.lemma_)\n",
    "        lemmatized_book[ix] = ' '.join(sp_sent)\n",
    "        \n",
    "    return lemmatized_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Books into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T19:57:09.548234Z",
     "start_time": "2019-05-20T19:57:09.476063Z"
    }
   },
   "outputs": [],
   "source": [
    "# full texts of male-authored books\n",
    "male_books = []\n",
    "# book file names, in the form \"book-title_book-author.txt\"\n",
    "male_book_list = []\n",
    "# specify file path to male-authored books\n",
    "path = \"/Users/winstonma4/Metis/project4/books/male/\"\n",
    "\n",
    "for file in glob.glob(path + \"*.txt\"):\n",
    "    male_book_list.append(file)\n",
    "    with open(file) as f:\n",
    "        male_books.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T19:32:42.974117Z",
     "start_time": "2019-05-20T19:32:42.941472Z"
    }
   },
   "outputs": [],
   "source": [
    "# full texts of female-authored books\n",
    "female_books = []\n",
    "# book file names, in the form \"book-title_book-author.txt\"\n",
    "female_book_list = []\n",
    "# specify file path to female-authored books\n",
    "path = \"/Users/winstonma4/Metis/project4/books/female/\"\n",
    "\n",
    "for file in glob.glob(path + \"*.txt\"):\n",
    "    female_book_list.append(file)\n",
    "    with open(file) as f:\n",
    "        female_books.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and Split Books into Pages (20 Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T20:34:28.594519Z",
     "start_time": "2019-05-20T20:09:38.158781Z"
    }
   },
   "outputs": [],
   "source": [
    "# fully-preprocessed texts of male-authored books\n",
    "male_pages = []\n",
    "# DataFrame containing documents with corresponding book title and author \n",
    "male_pages_df = pd.DataFrame(columns=['documents', 'book', 'author'])\n",
    "\n",
    "for ix, book in enumerate(male_books):\n",
    "    temp_book = preprocess_sent(book)\n",
    "    male_pages.extend(temp_book)\n",
    "    male_pages_df = pd.concat([male_pages_df, pd.DataFrame({'documents': temp_book, \n",
    "                  'book':male_book_list[ix].split('_')[0],    \n",
    "                  'author':male_book_list[ix].split('_')[1]})])\n",
    "male_pages_df['author_gender'] = 'male'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T19:48:28.426504Z",
     "start_time": "2019-05-20T19:32:51.691856Z"
    }
   },
   "outputs": [],
   "source": [
    "# fully-preprocessed texts of female-authored books\n",
    "female_pages = []\n",
    "# DataFrame containing documents with corresponding book title and author\n",
    "female_pages_df = pd.DataFrame(columns=['documents', 'book', 'author'])\n",
    "for ix, book in enumerate(female_books):\n",
    "    temp_book = preprocess_sent(book)\n",
    "    female_pages.extend(temp_book)\n",
    "    female_pages_df = pd.concat([female_pages_df, pd.DataFrame({'documents': temp_book, \n",
    "                  'book':female_book_list[ix].split('_')[0], \n",
    "                  'author':female_book_list[ix].split('_')[1]})])\n",
    "female_pages_df['author_gender'] = 'female'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master DataFrame with all books, titles, authors, and author genders\n",
    "df = pd.concat([male_pages_df, female_pages_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(df, 'all_books_df.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
